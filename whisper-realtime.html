<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Real-time Transcription</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #status {
            margin-top: 20px;
            padding: 10px;
            border-radius: 5px;
        }
        #transcription {
            margin-top: 20px;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 5px;
            min-height: 200px;
            max-height: 400px;
            overflow-y: auto;
        }
        button {
            padding: 10px 15px;
            margin-right: 10px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        #startBtn {
            background-color: #4CAF50;
            color: white;
        }
        #stopBtn {
            background-color: #f44336;
            color: white;
        }
        .controls {
            margin-top: 20px;
        }
        select {
            padding: 8px;
            border-radius: 5px;
        }
    </style>
    <!-- Favicon to prevent 404 error -->
    <link rel="icon" href="data:,">
</head>
<body>
    <h1>Whisper Real-time Transcription</h1>
    
    <div class="controls">
        <label for="model">Model:</label>
        <select id="model">
            <option value="tiny">Tiny (fastest)</option>
            <option value="base" selected>Base</option>
            <option value="small">Small</option>
            <option value="medium">Medium</option>
            <option value="large">Large (most accurate)</option>
        </select>

        <label for="language">Language:</label>
        <select id="language">
            <option value="auto" selected>Auto-detect</option>
            <option value="en">English</option>
            <option value="es">Spanish</option>
            <option value="fr">French</option>
            <option value="de">German</option>
            <!-- Add more languages as needed -->
        </select>
    </div>

    <div class="controls">
        <button id="startBtn">Start Transcription</button>
        <button id="stopBtn" disabled>Stop Transcription</button>
        <button id="clearBtn">Clear Transcription</button>
    </div>

    <div id="status">Status: Ready</div>
    <div id="info">
        <p><strong>Note:</strong> This page must be served through a web server to work properly.</p>
        <p>You can run the included server with: <code>python server.py</code> in the folder containing this file.</p>
        <p>Then access it at: <a href="http://localhost:8000/whisper-realtime.html">http://localhost:8000/whisper-realtime.html</a></p>
    </div>
    
    <div id="transcription"></div>

    <!-- Load dependencies in specific order -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.15.1/dist/ort.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.5.0/dist/transformers.min.js"></script>
    
    <script>
        // Global variables
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let recordingInterval = null;
        let whisperPipeline = null;
        let isProcessing = false;
        
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const clearBtn = document.getElementById('clearBtn');
        const statusElement = document.getElementById('status');
        const transcriptionElement = document.getElementById('transcription');
        const modelSelect = document.getElementById('model');
        const languageSelect = document.getElementById('language');
        
        // Wait for the libraries to be properly loaded
        function checkDependencies() {
            return new Promise((resolve, reject) => {
                // Check if ort is defined
                if (typeof ort === 'undefined') {
                    reject(new Error('ONNX Runtime not loaded properly'));
                    return;
                }
                
                // Check if Transformers is defined
                if (typeof Transformers === 'undefined') {
                    reject(new Error('Transformers library not loaded properly'));
                    return;
                }
                
                resolve();
            });
        }
        
        // Initialize application
        async function init() {
            try {
                await checkDependencies();
                
                // Configure the environment
                Transformers.env.useBrowserCache = true;
                Transformers.env.allowLocalModels = false;
                
                // Check browser support
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    statusElement.textContent = "Status: Your browser doesn't support audio recording";
                    startBtn.disabled = true;
                    return;
                }

                // Everything is loaded, set up event listeners
                startBtn.addEventListener('click', async () => {
                    await initWhisperModel();
                });

                stopBtn.addEventListener('click', stopRecording);
                
                clearBtn.addEventListener('click', () => {
                    transcriptionElement.innerHTML = "";
                });
                
                statusElement.textContent = "Status: Ready to start transcription";
                
            } catch (error) {
                console.error("Initialization error:", error);
                statusElement.textContent = `Error initializing: ${error.message}`;
                startBtn.disabled = true;
            }
        }
        
        // Initialize whisper model
        async function initWhisperModel() {
            try {
                statusElement.textContent = "Status: Loading Whisper model...";
                startBtn.disabled = true;
                
                const modelSize = modelSelect.value;
                const modelName = `Xenova/whisper-${modelSize}`;
                
                console.log(`Loading model: ${modelName}`);
                
                // Use a custom progress callback to update the UI
                const progress_callback = function(progress) {
                    if (progress.status === 'progress') {
                        const percent = Math.round(progress.progress * 100);
                        statusElement.textContent = `Status: Loading model... ${percent}%`;
                    } else if (progress.status === 'done') {
                        statusElement.textContent = "Status: Model initialized, preparing transcription...";
                    } else {
                        statusElement.textContent = `Status: ${progress.status}`;
                    }
                };
                
                // Create the pipeline with proper error handling
                whisperPipeline = await Transformers.pipeline('automatic-speech-recognition', modelName, {
                    quantized: true,
                    progress_callback: progress_callback,
                });
                
                statusElement.textContent = "Status: Whisper model loaded successfully";
                await startRecording();
                return true;
            } catch (error) {
                statusElement.textContent = `Status: Error initializing model - ${error.message}`;
                console.error("Error initializing model:", error);
                startBtn.disabled = false;
                return false;
            }
        }

        // Process audio with Whisper
        async function processAudio(audioBlob) {
            try {
                if (!whisperPipeline || isProcessing) {
                    console.warn("Model not ready or already processing, skipping");
                    return;
                }
                
                isProcessing = true;
                statusElement.textContent = "Status: Processing audio...";
                
                // Convert blob to array buffer
                const arrayBuffer = await audioBlob.arrayBuffer();
                
                // Convert to audio data
                const audioData = await prepareAudioData(arrayBuffer);
                
                const options = {};
                if (languageSelect.value !== 'auto') {
                    options.language = languageSelect.value;
                }
                
                // Run transcription
                const result = await whisperPipeline(audioData, options);
                
                if (result && result.text && result.text.trim() !== "") {
                    const paragraph = document.createElement('p');
                    paragraph.textContent = result.text;
                    transcriptionElement.appendChild(paragraph);
                    transcriptionElement.scrollTop = transcriptionElement.scrollHeight;
                    statusElement.textContent = "Status: Transcription updated";
                }
                
                isProcessing = false;
            } catch (error) {
                isProcessing = false;
                statusElement.textContent = `Status: Error processing audio - ${error.message}`;
                console.error("Error processing audio:", error);
            }
        }
        
        // Convert audio array buffer to format needed by model
        async function prepareAudioData(arrayBuffer) {
            // Create audio context with error handling
            const AudioContext = window.AudioContext || window.webkitAudioContext;
            if (!AudioContext) {
                throw new Error("AudioContext not supported by this browser");
            }
            
            const audioContext = new AudioContext();
            
            try {
                // Decode audio data
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Check if OfflineAudioContext is supported
                if (!window.OfflineAudioContext) {
                    throw new Error("OfflineAudioContext not supported by this browser");
                }
                
                // Prepare for model input - resample to 16kHz if needed
                const offlineContext = new OfflineAudioContext({
                    numberOfChannels: 1,
                    length: Math.ceil(audioBuffer.duration * 16000),
                    sampleRate: 16000
                });
                
                const source = offlineContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(offlineContext.destination);
                source.start(0);
                
                const renderedBuffer = await offlineContext.startRendering();
                const audioData = renderedBuffer.getChannelData(0);
                
                return audioData;
            } catch (error) {
                console.error("Audio processing error:", error);
                throw new Error(`Failed to process audio: ${error.message}`);
            } finally {
                // Close the audio context to free up resources
                if (audioContext.state !== 'closed') {
                    await audioContext.close();
                }
            }
        }

        // Start recording from the microphone
        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Select the correct MIME type based on browser support
                const mimeType = getSupportedMimeType();
                console.log(`Using MIME type: ${mimeType}`);
                
                const options = mimeType ? { mimeType } : {};
                mediaRecorder = new MediaRecorder(stream, options);
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };
                
                // Start the media recorder
                mediaRecorder.start(1000); // Collect data every second
                isRecording = true;
                
                // Set up interval to process audio chunks
                recordingInterval = setInterval(() => {
                    if (audioChunks.length > 0 && !isProcessing) {
                        const audioBlob = new Blob(audioChunks);
                        processAudio(audioBlob);
                        audioChunks = []; // Reset for next chunk
                    }
                }, 5000); // Process every 5 seconds
                
                startBtn.disabled = true;
                stopBtn.disabled = false;
                statusElement.textContent = "Status: Recording...";
            } catch (error) {
                statusElement.textContent = `Status: Microphone access error - ${error.message}`;
                console.error("Error accessing microphone:", error);
                startBtn.disabled = false;
            }
        }
        
        // Helper function to get supported MIME type
        function getSupportedMimeType() {
            const types = [
                'audio/webm',
                'audio/webm;codecs=opus',
                'audio/ogg;codecs=opus',
                'audio/wav',
                'audio/mp4'
            ];
            
            for (const type of types) {
                if (MediaRecorder.isTypeSupported(type)) {
                    return type;
                }
            }
            
            return ''; // Browser will use default type
        }

        // Stop recording
        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                
                // Process any remaining audio
                if (audioChunks.length > 0) {
                    const audioBlob = new Blob(audioChunks);
                    processAudio(audioBlob);
                    audioChunks = [];
                }
                
                // Clear any tracks in the stream
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
                
                clearInterval(recordingInterval);
                isRecording = false;
                startBtn.disabled = false;
                stopBtn.disabled = true;
                statusElement.textContent = "Status: Recording stopped";
            }
        }

        // Initialize the app when the page loads
        document.addEventListener('DOMContentLoaded', init);
    </script>
</body>
</html>
